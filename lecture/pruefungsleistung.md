# üß≠  Pr√ºfungsleistung ‚Äì Software Engineering Projekt

**Leistungsform:** Projektarbeit in Gruppen (3‚Äì4 Studierende)  
**Ziel:** Entwicklung eines eigenst√§ndigen Software-Services als Beitrag zur Campus-Web-App  
**Bewertung:** 100 Punkte insgesamt  

## üßÆ Notenfindung

Zur Notenvergabe bestehen zwei Varianten, die vor der Bewertung im Team abgestimmt werden m√ºssen:

### **Option A ‚Äì Gemeinsame Bewertung**
Alle Teammitglieder erhalten **die gleiche Note**.  
Dies ist der Standardfall und setzt voraus, dass das Projekt als gemeinschaftliche Leistung erbracht wurde.

### **Option B ‚Äì Individuelle Bewertung**
Jede Person erh√§lt **eine individuelle Note** auf Basis ihres dokumentierten Beitrags.  
Hierf√ºr muss das Team **eine klare Aufteilung der Verantwortlichkeiten** (z. B. in der Dokumentation oder im Repository) angeben:  
- Wer hat welche Komponenten entwickelt (Frontend, Backend, Datenbank, CI/CD, Tests)?  
- Welche Aufgaben wurden √ºbernommen (z. B. Architektur, DevOps, Projektmanagement)?  
- Wie wurde im Team kommuniziert und koordiniert?


## üìä Bewertungsmatrix

| Kategorie                               | Beschreibung                                          | Gewichtung | Bewertungskriterien |
|-----------------------------------------|-------------------------------------------------------|------------|----------------------|
| **1. Technische Umsetzung**             | Funktionalit√§t, Codequalit√§t, Architektur             | **30 %**   | - Funktioniert der Service zuverl√§ssig?<br>- Ist die Architektur modular und verst√§ndlich?<br>- Wurde sauber programmiert (Clean Code, Namensgebung, Struktur)?<br>- Wurden Frameworks und Libraries sinnvoll eingesetzt?<br>- Wurde eine Datenbank fachgerecht modelliert und angebunden? |
| **2. Software Engineering Prozess**     | Methodisches Vorgehen, Versionskontrolle, CI/CD, Tests | **20 %**   | - Wurde mit Git gearbeitet (regelm√§√üige Commits, Branching, Merges)?<br>- Sind automatisierte Tests vorhanden?<br>- Wurde dokumentiert, wie gebaut und deployed wird?<br>- Wurde in Jira / GitHub Issues / Confluence ein nachvollziehbarer Prozess gef√ºhrt? |
| **3. Frontend & Usability**             | Benutzeroberfl√§che, UX/UI-Design, Konsistenz          | **5 %**    | - Ist das UI klar und funktionsgerecht?<br>- Wurde auf Konsistenz und Responsivit√§t geachtet?<br>- Ist die Nutzerf√ºhrung verst√§ndlich? |
| **4. Teamarbeit & Kommunikation**       | Organisation, Rollen, Zusammenarbeit, Pr√§sentation    | **10 %**   | - Wie gut funktionierte die Teamarbeit?<br>- Wurde Arbeit aufgeteilt und abgestimmt?<br>- Wie ist die gemeinsame Pr√§sentation des Projekts?<br>- Sind Verantwortlichkeiten dokumentiert? |
| **5. Dokumentation & Reflexion**        | Technische & fachliche Nachvollziehbarkeit            | **20 %**   | - Gibt es eine saubere README / Confluence-Seite?<br>- Architekturdiagramme (z. B. C4 oder UML)?<br>- Beschreibung der API-Schnittstellen?<br>- Reflexion √ºber Herausforderungen und Learnings? |
| **6. Projektumfang & Schwierigkeitsgrad** | Realistischer, aber ambitionierter Umfang des Projekts | **15 %**   | - Komplexit√§t der gew√§hlten Aufgabe (z. B. API-Integration, Authentifizierung, Datenmodellierung)<br>- Eigenst√§ndigkeit und Innovationsgrad<br>- Angemessenes Verh√§ltnis von Aufwand zu Qualit√§t |

## ‚è∞ Abgabetermin
24.12.2025, 12:00 Uhr (GMT+1)
